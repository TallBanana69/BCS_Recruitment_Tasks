{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa545704-71a7-4868-b63e-a2fc3db5f9e0",
   "metadata": {},
   "source": [
    "Importing and downloading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb36f7a-df71-4686-9a5d-9c214596743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3706f7a1-5cd8-4e15-bb5a-7c62fb1ba9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac6f1a-751f-4847-8975-1d12bdc459ab",
   "metadata": {},
   "source": [
    "Extracting text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ca1468-0a3f-4ef8-aa62-4bbea5518e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23932e46-0adb-4eb7-9d6d-566bb52c8f6a",
   "metadata": {},
   "source": [
    "Lowecasing, removing punctuations, tokeninzing, removing stopwords, Lemmatizing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17c5b4f-3708-4ff9-be7f-31e244ff3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    " \n",
    "    text = text.lower()\n",
    "    \n",
    "   \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "  \n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "   \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f18a69-5c69-42ba-80af-2df407d3db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_and_pos_tagging(tokens):\n",
    "\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "\n",
    "    named_entities = nltk.ne_chunk(pos_tags, binary=True)\n",
    "    \n",
    "    return named_entities, pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d86ab0c5-9b56-4188-b7eb-344f8a89ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_chunks):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(text_chunks)\n",
    "    return tfidf_matrix, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e63d92-b69e-4104-b596-b1714d174754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query_vector, tfidf_matrix, text_chunks, top_n=3):\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_similar_indices = similarities.argsort()[::-1][:top_n]\n",
    "    top_similar_chunks = [text_chunks[i] for i in top_similar_indices]\n",
    "    return top_similar_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6efbc8e8-eb31-402f-a19e-41621a073bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_path = \"./Desktop/the_hitchhiker_s_guide_to_the_galaxy.pdf\"\n",
    "user_query = \"Why are humans unhappy?\"\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "text_chunks = sent_tokenize(pdf_text)\n",
    "\n",
    "preprocessed_chunks = [' '.join(preprocess_text(chunk)) for chunk in text_chunks]\n",
    "\n",
    "\n",
    "tfidf_matrix, tfidf_vectorizer = generate_embeddings(preprocessed_chunks)\n",
    "\n",
    "\n",
    "preprocessed_query = ' '.join(preprocess_text(user_query))\n",
    "query_vector = tfidf_vectorizer.transform([preprocessed_query])\n",
    "\n",
    "top_similar_chunks = semantic_search(query_vector, tfidf_matrix, text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dfff1e9-e198-4ef4-a987-53e650eef0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What is your name, human?\"\n",
      "This planet has - or rather had - a problem, which was this: most \n",
      "of the people on it were unhappy for pretty much of the time.\n",
      "I've just had an unhappy \n",
      "love affair, so I don't see why anybody else should have a good \n",
      "time.\n"
     ]
    }
   ],
   "source": [
    "for chunk in top_similar_chunks:\n",
    "    print(chunk)\n",
    "    \n",
    "\n",
    "    tokens = word_tokenize(chunk)\n",
    "    named_entities, pos_tags = ner_and_pos_tagging(tokens)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
